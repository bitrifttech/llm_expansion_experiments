# Continual Learning Utilities

This package provides shared utilities for analyzing models and calculating metrics across different continual learning experiments.

## ModelAnalyzer

The `ModelAnalyzer` class provides comprehensive analysis of transformer models including:

### Features

- **ğŸ“Š Model Overview**: Basic model information, architecture type, device, and data type
- **ğŸ”¢ Parameter Analysis**: Total, trainable, and frozen parameter counts with percentages
- **ğŸ—ï¸ Architecture Details**: Layer counts, dimensions, and model-specific information
- **ğŸ”§ Custom Component Detection**: Automatically detects LoRA, ExpandedFFN, and other custom layers
- **âš¡ Efficiency Metrics**: Parameters per MB, memory per parameter, trainable ratios
- **ğŸ¯ Layer Breakdown**: Detailed analysis of individual layers with parameter counts
- **ğŸ”„ Model Comparison**: Before/after analysis for model modifications

### Usage

#### Basic Analysis
```python
from utils.model_analyzer import ModelAnalyzer, analyze_model

# Simple analysis
model = T5ForConditionalGeneration.from_pretrained("Salesforce/codet5-small")
analysis = analyze_model(model, "CodeT5-Small", detailed=True)

# Using the class directly
analyzer = ModelAnalyzer(model, "My Model")
analysis = analyzer.analyze(detailed=True)
```

#### Model Comparison
```python
# Compare original vs modified model
original_analyzer = ModelAnalyzer(original_model, "Original")
modified_analyzer = ModelAnalyzer(modified_model, "Modified")

comparison = original_analyzer.compare_with(modified_analyzer, "My Modification")
```

### Output Example

```
============================================================
ğŸ” ANALYZING MODEL: CodeT5-Small (Base)
============================================================

ğŸ“Š MODEL OVERVIEW
â”œâ”€â”€ Model: CodeT5-Small (Base)
â”œâ”€â”€ Type: T5ForConditionalGeneration
â”œâ”€â”€ Architecture: Encoder-Decoder
â”œâ”€â”€ Device: mps:0
â””â”€â”€ Data Type: torch.float32

ğŸ”¢ PARAMETER SUMMARY
â”œâ”€â”€ Total Parameters: 60,492,288
â”œâ”€â”€ Trainable Parameters: 60,492,288 (100.00%)
â”œâ”€â”€ Frozen Parameters: 0 (0.00%)
â””â”€â”€ Memory Usage: 461.52 MB

ğŸ—ï¸ ARCHITECTURE DETAILS
â”œâ”€â”€ Encoder Layers: 6
â”œâ”€â”€ Decoder Layers: 6
â”œâ”€â”€ Model Dimension: 512
â””â”€â”€ FFN Dimension: 2048

âš¡ EFFICIENCY METRICS
â”œâ”€â”€ Parameters per MB: 131,072
â”œâ”€â”€ Memory per Parameter: 8.00 bytes
â””â”€â”€ Trainable Ratio: 100.00%

ğŸ¯ TOP TRAINABLE LAYERS
â”œâ”€â”€ shared: 16,435,200 params (Embedding)
â”œâ”€â”€ lm_head: 16,435,200 params (Linear)
â”œâ”€â”€ encoder.block.0.layer.1.DenseReluDense.wi: 1,048,576 params (Linear)
â”œâ”€â”€ encoder.block.0.layer.1.DenseReluDense.wo: 1,048,576 params (Linear)
â””â”€â”€ encoder.block.1.layer.1.DenseReluDense.wi: 1,048,576 params (Linear)
    ... and 127 more trainable layers
============================================================
```

### Custom Component Detection

The analyzer automatically detects and highlights custom components:

- **ExpandedFFN**: FFN expansion layers
- **LoRA**: Low-rank adaptation layers
- **PeftModel**: Parameter-efficient fine-tuning models
- **AdaLoRA**: Adaptive LoRA layers
- **IA3**: Infused Adapter layers

### Integration in Experiments

The ModelAnalyzer is integrated into all continual learning experiments:

1. **Base Model Analysis**: Analyze the original model before modifications
2. **Expansion Analysis**: Show detailed comparison after adding parameters
3. **Training Analysis**: Track parameter changes during training
4. **Cross-Experiment Consistency**: Ensure consistent metrics across all approaches

### Benefits

- **ğŸ”„ Consistency**: Standardized analysis across all experiments
- **ğŸ“ˆ Insights**: Detailed parameter and memory breakdowns
- **ğŸ¯ Focus**: Highlights trainable components and custom layers
- **âš¡ Efficiency**: Quick comparison of different approaches
- **ğŸ”§ Debugging**: Easy identification of model modifications

## Future Extensions

- **Memory Profiling**: Runtime memory usage tracking
- **FLOP Counting**: Computational complexity analysis
- **Gradient Analysis**: Gradient flow and magnitude tracking
- **Performance Metrics**: Training speed and convergence analysis 